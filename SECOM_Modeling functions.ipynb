{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b287f2c4-d868-4f39-bdfa-bf5aaae0a519",
   "metadata": {},
   "source": [
    "## Preparation for the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1722cec7-3185-48e1-97cc-bfd0400f97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: imbalanced-learn==0.11.0 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (from scikit-learn==1.3.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (from scikit-learn==1.3.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yi-tsenlin\\anaconda3\\lib\\site-packages (from scikit-learn==1.3.2) (3.5.0)\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install scikit-learn==1.3.2 imbalanced-learn==0.11.0\n",
    "!conda install -c conda-forge boruta_py -y\n",
    "!conda install -c conda-forge imbalanced-learn=0.10.1 -y\n",
    "import numpy as np\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.bool = bool\n",
    "np.object = object\n",
    "np.str = str\n",
    "import pandas as pd \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7f926-9393-4665-b7ff-61738f677a6e",
   "metadata": {},
   "source": [
    "### Build the dicitonary for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ffebdca-327d-4a41-a2d0-632c04f8436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"K Nearest Neighbour\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "balancers = {\n",
    "    \"SMOTE\": lambda: SMOTE(sampling_strategy=1.0, random_state=42),\n",
    "    \"ADASYN\": lambda: ADASYN(sampling_strategy=1.0, random_state=42),\n",
    "    \"RandomOverSampler\": lambda: RandomOverSampler(sampling_strategy=1.0, random_state=42)\n",
    "}\n",
    "\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler,\n",
    "    \"MinMaxScaler\": MinMaxScaler,\n",
    "    \"No Scaling\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9398330-a301-4d31-86ae-fab43066b1ca",
   "metadata": {},
   "source": [
    "### Export format of Cols, results and a funcaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d3538e-e04f-48a5-b4ab-d39161e1af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the column structure\n",
    "columns = [\n",
    "    'imputer', 'balancer', 'scaler', 'model', 'accuracy',\n",
    "    'precision_class_-1', 'recall_class_-1', 'f1_class_-1',\n",
    "    'precision_class_1', 'recall_class_1', 'f1_class_1',\n",
    "    'macro_f1', 'weighted_f1',\n",
    "    'confusion_tp', 'confusion_tn', 'confusion_fp', 'confusion_fn',\n",
    "    'specificity_class_-1',\n",
    "    'notebook_id', 'timestamp',\n",
    "    'normality_test'  \n",
    "]\n",
    "# Example result (can be replaced with a loop or real data)\n",
    "results = [{\n",
    "    'imputer': 'imputer_name',\n",
    "    'balancer': 'balancer_name',\n",
    "    'scaler': 'scaler_name',\n",
    "    'model': 'model_name',\n",
    "    'accuracy': 0.796,\n",
    "    'precision_class_-1': 0.13,\n",
    "    'recall_class_-1': 0.29,\n",
    "    'f1_class_-1': 0.18,\n",
    "    'precision_class_1': 0.13,\n",
    "    'recall_class_1': 0.29,\n",
    "    'f1_class_1': 0.18,\n",
    "    'macro_f1': 0.53,\n",
    "    'weighted_f1': 0.83,\n",
    "    'notebook_id': \"Yitsen\",\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'normality_test': 0\n",
    "}]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "output_file = \"model_comparison_results.csv\"\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not os.path.exists(output_file):\n",
    "    # Save header with empty row if needed\n",
    "    pd.DataFrame(columns=columns).to_csv(output_file, index=False)\n",
    "    print(f\"Created empty results file with correct structure: {output_file}\")\n",
    "\n",
    "\n",
    "# Function to append results to CSV\n",
    "def append_results_to_csv(new_results, output_file=\"model_comparison_results.csv\"):\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    results_df = pd.DataFrame(new_results)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        existing = pd.read_csv(output_file)\n",
    "\n",
    "        id_cols = [\"notebook_id\", \"imputer\", \"scaler\", \"balancer\", \"model\"]\n",
    "        existing_filtered = existing[~existing[id_cols].apply(tuple, axis=1).isin(\n",
    "            results_df[id_cols].apply(tuple, axis=1)\n",
    "        )]\n",
    "\n",
    "        final = pd.concat([existing_filtered, results_df], ignore_index=True)\n",
    "    else:\n",
    "        final = results_df\n",
    "\n",
    "    final.to_csv(output_file, index=False)\n",
    "    print(f\"Updated {len(new_results)} rows in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc249d-e7a2-42fd-8484-168e7232b18e",
   "metadata": {},
   "source": [
    "### Set the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c543d9-b598-450c-a8e6-02692e9126fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Current working directory: C:\\Users\\Yi-TsenLin\\Desktop\\HTW\\25SOSE\\Data Analytics II\\00_Assignment\\Code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "code_path = \"./Code\"\n",
    "if os.path.exists(code_path):\n",
    "    os.chdir(code_path)\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "else:\n",
    "    print(f\"Path not found: {code_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce352f-ba1a-48a8-a2d7-25e0b3fd0a84",
   "metadata": {},
   "source": [
    "### Imputation in KNN (avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab96f22-e004-4fd7-ae22-88170e09de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#  KNNImputer Ôºà train_KNN as train setÔºâ\n",
    "\n",
    "train = pd.read_csv(\"train_KNN.csv\")\n",
    "test = pd.read_csv(\"test_revised.csv\")\n",
    "\n",
    "# sepsration of feature and label\n",
    "X_train = train.drop(columns=['label'])\n",
    "y_train = train['label']\n",
    "X_test = test.drop(columns=['label'])\n",
    "y_test = test['label']\n",
    "\n",
    "# Remove non-numeric fields (especially timestamps)\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "X_train = X_train.drop(columns=non_numeric_cols)\n",
    "X_test = X_test.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Fit only on train !!!imporatant!!!\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)  #transform only\n",
    "\n",
    "# merge \n",
    "train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "train_imputed['label'] = y_train.values\n",
    "test_imputed = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "test_imputed['label'] = y_test.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce69317-0cbe-498d-9551-a53d41c7c116",
   "metadata": {},
   "source": [
    "### Feature Selection in KNN - Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a470932e-0098-4506-a759-d2e9b030e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File savedÔºöKNN_train_boruta.csv, KNN_test_boruta.csv\n"
     ]
    }
   ],
   "source": [
    "# Drop 'timestamp' if exists\n",
    "for df in [train_imputed, test_imputed]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "X_train = train_imputed.drop(columns=[\"label\"])\n",
    "y_train = train_imputed[\"label\"]\n",
    "\n",
    "# Fit Boruta\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta = BorutaPy(estimator=rf, n_estimators='auto', random_state=42)\n",
    "boruta.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Select features\n",
    "selected_cols = X_train.columns[boruta.support_]\n",
    "X_train_selected = X_train[selected_cols]\n",
    "X_test_selected = test_imputed[selected_cols]\n",
    "\n",
    "#  merge target into the original data set\n",
    "train_selected = X_train_selected.copy()\n",
    "train_selected[\"label\"] = y_train.values\n",
    "\n",
    "test_selected = X_test_selected.copy()\n",
    "test_selected[\"label\"] = y_test.loc[X_test_selected.index].values\n",
    "\n",
    "# export CSV\n",
    "train_selected.to_csv(\"KNN_train_boruta.csv\", index=False)\n",
    "test_selected.to_csv(\"KNN_test_boruta.csv\", index=False)\n",
    "\n",
    "print(\"File savedÔºöKNN_train_boruta.csv, KNN_test_boruta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92c2aa-8afb-4889-8c56-313c7684cc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1b4bfa-e70f-46d1-9934-8abb326f1c4e",
   "metadata": {},
   "source": [
    "### Imputation in MICE (avoid data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1cfb3c-be80-4568-ab9b-ec6b52f417b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MICE Ôºà train_MICE as train setÔºâ\n",
    "\n",
    "train = pd.read_csv(\"train_MICE.csv\")\n",
    "test = pd.read_csv(\"test_revised.csv\")\n",
    "\n",
    "# sepsration of feature and label\n",
    "X_train = train.drop(columns=['label'])\n",
    "y_train = train['label']\n",
    "X_test = test.drop(columns=['label'])\n",
    "y_test = test['label']\n",
    "\n",
    "# Remove non-numeric fields (especially timestamps)\n",
    "non_numeric_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "X_train = X_train.drop(columns=non_numeric_cols)\n",
    "X_test = X_test.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Fit only on train !!!imporatant!!!\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)  #transform only\n",
    "\n",
    "# merge \n",
    "train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "train_imputed['label'] = y_train.values\n",
    "test_imputed = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "test_imputed['label'] = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7f7b9-4934-42c0-a358-749a4b05b7aa",
   "metadata": {},
   "source": [
    "### Feature Selection in MICE - Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442cc6a5-32c5-4cd6-ab81-82733cd7ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File savedÔºöMICE_train_boruta.csv, MICE_test_boruta.csv\n"
     ]
    }
   ],
   "source": [
    "# Drop 'timestamp' if exists\n",
    "for df in [train_imputed, test_imputed]:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Split into features and target\n",
    "X_train = train_imputed.drop(columns=[\"label\"])\n",
    "y_train = train_imputed[\"label\"]\n",
    "\n",
    "# Fit Boruta\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta = BorutaPy(estimator=rf, n_estimators='auto', random_state=42)\n",
    "boruta.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Select features\n",
    "selected_cols = X_train.columns[boruta.support_]\n",
    "X_train_selected = X_train[selected_cols]\n",
    "X_test_selected = test_imputed[selected_cols]\n",
    "\n",
    "#  merge target into the original data set\n",
    "train_selected = X_train_selected.copy()\n",
    "train_selected[\"label\"] = y_train.values\n",
    "\n",
    "test_selected = X_test_selected.copy()\n",
    "test_selected[\"label\"] = y_test.loc[X_test_selected.index].values\n",
    "\n",
    "# export CSV\n",
    "train_selected.to_csv(\"MICE_train_boruta.csv\", index=False)\n",
    "test_selected.to_csv(\"MICE_test_boruta.csv\", index=False)\n",
    "\n",
    "print(\"File savedÔºöMICE_train_boruta.csv, MICE_test_boruta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb184fd-1cb4-4309-8f50-03c8b2021d9b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585a869e-26c6-426c-9ee7-612130fd4066",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b258edc1-28bc-4efa-a4fa-63b30e792a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "def run_model_experiment(X_train, X_test, y_train, y_test, balancer_name, models_dict, scaler, notebook_id, imputer_name=\"KNN\"):\n",
    "    results = []\n",
    "\n",
    "    # Apply scaler if needed\n",
    "    if scaler:\n",
    "        scaler_instance = scaler()\n",
    "        X_train = scaler_instance.fit_transform(X_train)\n",
    "        X_test = scaler_instance.transform(X_test)\n",
    "        scaler_name = scaler.__name__\n",
    "    else:\n",
    "        scaler_name = \"None\"\n",
    "\n",
    "    # Apply balancing\n",
    "    if balancer_name == \"Manual_80_20\":\n",
    "        X_res, y_res = resample_to_ratio(pd.DataFrame(X_train), pd.Series(y_train), positive_ratio=0.8)\n",
    "    else:\n",
    "        balancer_instance = balancers[balancer_name]() \n",
    "        X_res, y_res = balancer_instance.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Loop through models\n",
    "    for model_name, model in models_dict.items():\n",
    "        X_res_model = X_res.copy()\n",
    "        X_test_model = X_test.copy()\n",
    "\n",
    "        # Normality testing and transformation (only for specific models)\n",
    "        if model_name in [\"Logistic Regression\", \"Linear Discriminant Analysis\"]:\n",
    "            print(f\"[{model_name}] Checking normality & applying Yeo-Johnson...\")\n",
    "\n",
    "            # Á¢∫‰øùÊòØ numpy\n",
    "            if isinstance(X_res_model, pd.DataFrame):\n",
    "                X_res_model = X_res_model.values\n",
    "                X_test_model = X_test_model.values\n",
    "\n",
    "            transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            columns_to_transform = []\n",
    "            for i in range(X_res_model.shape[1]):\n",
    "                col_data = X_res_model[:, i]\n",
    "                _, p_value = lilliefors(col_data)\n",
    "                if p_value < 0.05:\n",
    "                    columns_to_transform.append(i)\n",
    "\n",
    "            if columns_to_transform:\n",
    "                X_res_model[:, columns_to_transform] = transformer.fit_transform(X_res_model[:, columns_to_transform])\n",
    "                X_test_model[:, columns_to_transform] = transformer.transform(X_test_model[:, columns_to_transform])\n",
    "                print(f\"Yeo-Johnson applied to {len(columns_to_transform)} features.\")\n",
    "            else:\n",
    "                print(\"All features passed normality test. No transformation needed.\")\n",
    "\n",
    "        # Ê®°ÂûãË®ìÁ∑¥ËàáÈ†êÊ∏¨\n",
    "        model.fit(X_res_model, y_res)\n",
    "        y_pred = model.predict(X_test_model)\n",
    "\n",
    "        # Convert y_test and y_pred to float for consistency\n",
    "        y_test_str = y_test.astype(float)\n",
    "        y_pred_str = pd.Series(y_pred).astype(float)\n",
    "\n",
    "         # Accuracy and classification report\n",
    "        acc = accuracy_score(y_test_str, y_pred_str)\n",
    "        report = classification_report(y_test_str, y_pred_str, output_dict=True)\n",
    "\n",
    "        # Confusion matrix\n",
    "        labels = [-1.0, 1.0]\n",
    "        try: \n",
    "        # if '-1.0' in y_test_str.unique() else ['-1', '1']\n",
    "            cm = confusion_matrix(y_test_str, y_pred_str, labels=labels)\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()  \n",
    "            else: \n",
    "                tn = fp = fn = tp = 0\n",
    "        except Exception as e:\n",
    "            print(\"Error computing confusion matrix:\", e)\n",
    "            tn = fp = fn = tp = 0\n",
    "\n",
    "        # Specificity\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "        report = classification_report(y_test_str, y_pred_str, labels=labels, output_dict=True)\n",
    "\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test_str, y_pred_str))\n",
    "\n",
    "        results.append({\n",
    "            'imputer': imputer_name,\n",
    "            'balancer': balancer_name,\n",
    "            'scaler': scaler_name,\n",
    "            'model': model_name,\n",
    "            'accuracy': acc,\n",
    "            'precision_class_-1': report.get('-1.0', report.get('-1', {})).get('precision', 0),\n",
    "            'recall_class_-1': report.get('-1.0', report.get('-1', {})).get('recall', 0),\n",
    "            'f1_class_-1': report.get('-1.0', report.get('-1', {})).get('f1-score', 0),\n",
    "            'precision_class_1': report.get('1.0', report.get('1', {})).get('precision', 0),\n",
    "            'recall_class_1': report.get('1.0', report.get('1', {})).get('recall', 0),\n",
    "            'f1_class_1': report.get('1.0', report.get('1', {})).get('f1-score', 0),\n",
    "            'macro_f1': report['macro avg']['f1-score'],\n",
    "            'weighted_f1': report['weighted avg']['f1-score'],\n",
    "            'confusion_tp': tp,\n",
    "            'confusion_tn': tn,\n",
    "            'confusion_fp': fp,\n",
    "            'confusion_fn': fn,\n",
    "            'specificity_class_-1': specificity,\n",
    "            'notebook_id': notebook_id,\n",
    "        })\n",
    "        \n",
    "    print(\"y_test_str:\", pd.Series(y_test_str).value_counts().to_dict())\n",
    "    print(\"y_pred_str:\", pd.Series(y_pred_str).value_counts().to_dict())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6c81a-78b8-4716-9df7-d2f6f0cf5377",
   "metadata": {},
   "source": [
    "### Loop for iterating each result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a97765b-21d0-44d1-9b43-794a46cc843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline(dataset_list, models_dict, scalers, balancers, output_file=\"model_comparison_results.csv\"):\n",
    "    \"\"\"\n",
    "    Execute the complete model training process and write the results to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_list: list of tuples ‚Üí [(train_df, test_df, \"KNN\"), (train_df, test_df, \"MICE\")]\n",
    "    - models_dict: Model list\n",
    "    - scalers: Standardization methods\n",
    "    - balancers: Resampling methods\n",
    "    - output_file: Output file path\n",
    "    \"\"\"\n",
    "    for train_df, test_df, imputer_name in dataset_list:\n",
    "        notebook_id = f\"Yitsen_{imputer_name}_CF\"\n",
    "        print(f\"\\nüìÇ Running pipeline for imputer = {imputer_name}\")\n",
    "\n",
    "        # Iterate through all scaler √ó balancer combinations\n",
    "        for scaler_name, scaler_class in scalers.items():\n",
    "            for balancer_name in balancers:\n",
    "                all_results = run_model_experiment(\n",
    "                    X_train=train_df.drop(columns=['label']),\n",
    "                    X_test=test_df.drop(columns=['label']),\n",
    "                    y_train=train_df['label'],\n",
    "                    y_test=test_df['label'],\n",
    "                    balancer_name=balancer_name,\n",
    "                    models_dict=models_dict,\n",
    "                    scaler=scaler_class,\n",
    "                    notebook_id=notebook_id,\n",
    "                    imputer_name=imputer_name\n",
    "                )\n",
    "                append_results_to_csv(all_results, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12d15c-b47f-4f3d-8710-f65829bf5e22",
   "metadata": {},
   "source": [
    "### Tuples for the two imputation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284df9e-0fac-4f79-877e-a2b9507f4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data you have divided up\n",
    "train_knn = pd.read_csv(\"KNN_train_boruta.csv\")\n",
    "test_knn = pd.read_csv(\"KNN_test_boruta.csv\")\n",
    "\n",
    "train_mice = pd.read_csv(\"MICE_train_boruta.csv\")\n",
    "test_mice = pd.read_csv(\"MICE_test_boruta.csv\")\n",
    "\n",
    "# Pass in list of (train_df, test_df, imputer_name)\n",
    "file_list = [\n",
    "    (train_knn, test_knn, \"KNN\"),\n",
    "    (train_mice, test_mice, \"MICE\")\n",
    "]\n",
    "\n",
    "# Run\n",
    "run_full_pipeline(file_list, models_dict, scalers, balancers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db6603-b62a-4bb4-b7ce-7b4d60d61095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1ea5a-f80b-4b85-b8c2-742e1e171f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
